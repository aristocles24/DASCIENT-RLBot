{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/yahoo-tradingview-top-gainers-kalman-v2-0?scriptVersionId=298132177\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"This notebook pulls **US large-cap market movers** from TradingView (via `pandas.read_html`) and then fetches OHLCV history from **Yahoo Finance** (via `yfinance`) for analysis, smoothing, and a lightweight forecast using a 2‑state Kalman Filter (price + velocity).\n\nDesign goals for this version:\n- Same data sources / retrieval protocols as the original (TradingView HTML table + Yahoo Finance via `yfinance`)\n- More dependable execution (retries, caching, validation)\n- More usable flow (clear entry points, modular functions, readable outputs)\n- More resource-aware (optional installs, minimal re-downloads)\n\n### See below for supplemental simple day forecast for \"Strong Buys\".\n\n> Note: TradingView page structure can change. If the table stops parsing, the fallback logic will explain what happened and how to proceed.\n","metadata":{}},{"cell_type":"code","source":"# --- (Optional) Install dependencies ---\n# If you're in a managed environment where installs are not allowed, you can comment this cell out.\n# Keeping installs in one place helps reproducibility.\nimport sys, subprocess, importlib\n\ndef _ensure(pkg: str, import_name: str | None = None):\n    name = import_name or pkg\n    try:\n        importlib.import_module(name)\n    except Exception:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\nfor pkg, name in [\n    (\"pandas\", \"pandas\"),\n    (\"numpy\", \"numpy\"),\n    (\"yfinance\", \"yfinance\"),\n    (\"plotly\", \"plotly\"),\n    (\"filterpy\", \"filterpy\"),\n    (\"requests\", \"requests\"),\n]:\n    _ensure(pkg, name)\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\n\nfrom filterpy.kalman import KalmanFilter\nimport plotly.graph_objects as go\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport time\nimport logging\nfrom typing import Optional, Tuple\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:29:58.747301Z","iopub.execute_input":"2025-12-17T05:29:58.747517Z","iopub.status.idle":"2025-12-17T05:30:10.507764Z","shell.execute_reply.started":"2025-12-17T05:29:58.747492Z","shell.execute_reply":"2025-12-17T05:30:10.506785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:30:10.510084Z","iopub.execute_input":"2025-12-17T05:30:10.510564Z","iopub.status.idle":"2025-12-17T05:30:10.514911Z","shell.execute_reply.started":"2025-12-17T05:30:10.510535Z","shell.execute_reply":"2025-12-17T05:30:10.51413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration / Entry Points ---\n\n@dataclass(frozen=True)\nclass Config:\n    # TradingView \"market movers\" table (same protocol: HTML table -> pandas.read_html)\n    gainers_url: str = \"https://www.tradingview.com/markets/stocks-usa/market-movers-large-cap/\"\n    # Pick the Nth row from the table (0 = top row)\n    gainer_rank: int = 0\n\n    # Yahoo Finance history config (same protocol: yfinance.download)\n    period: str = \"6mo\"\n    interval: str = \"1d\"\n\n    # Analysis / forecast\n    sma_window: int = 10\n    n_forecast: int = 10  # business days\n\n    # Reliability knobs\n    retries: int = 3\n    backoff_seconds: float = 1.2\n\n    # Local caching (avoids re-downloading during iterative runs)\n    cache_dir: str = \".cache_yf\"\n\nCFG = Config()\nCFG\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:30:10.515954Z","iopub.execute_input":"2025-12-17T05:30:10.516637Z","iopub.status.idle":"2025-12-17T05:30:10.539805Z","shell.execute_reply.started":"2025-12-17T05:30:10.516608Z","shell.execute_reply":"2025-12-17T05:30:10.538908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Helpers: logging, caching, robust fetching (TradingView + Yahoo) ---\n\nimport logging, time, re\nfrom pathlib import Path\nimport pandas as pd\nimport yfinance as yf\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n)\n\nCACHE_DIR = Path(CFG.cache_dir)\nCACHE_DIR.mkdir(parents=True, exist_ok=True)\n\n# -------------------------\n# Retry helper\n# -------------------------\ndef _retry(call, *, retries: int, backoff: float, what: str):\n    last_err = None\n    for i in range(retries):\n        try:\n            return call()\n        except Exception as e:\n            last_err = e\n            wait = backoff * (2 ** i)\n            logging.warning(\"%s failed (%s). retry %d/%d in %.1fs\", what, e, i + 1, retries, wait)\n            time.sleep(wait)\n    raise RuntimeError(f\"{what} failed after {retries} retries\") from last_err\n\n# -------------------------\n# TradingView Symbol parsing (fixes \"NVDANVIDIA Corporation\" -> \"NVDA\")\n# -------------------------\n_TICKER_CHARS = re.compile(r\"^[A-Z0-9.\\-=]+$\")\n_SUFFIX_WORDS = {\"INC\", \"INC.\", \"CORP\", \"CORP.\", \"CORPORATION\", \"LTD\", \"LTD.\", \"PLC\", \"CO\", \"CO.\"}\n\ndef _split_symbol_cell(s: str) -> tuple[str, str]:\n    \"\"\"\n    Convert TradingView mashed cells into (Ticker, Company).\n\n    Examples:\n      'NVDANVIDIA Corporation' -> ('NVDA', 'NVIDIA Corporation')\n      'AAPLApple Inc.'         -> ('AAPL', 'Apple Inc.')\n      'MSFTMicrosoft Corp.'    -> ('MSFT', 'Microsoft Corp.')\n      'NASDAQ:AAPLApple Inc.'  -> ('AAPL', 'Apple Inc.')\n      'BRK.BBerkshire ...'     -> ('BRK.B', 'Berkshire ...')\n    \"\"\"\n    if s is None:\n        return \"\", \"\"\n    raw = str(s).strip()\n    if not raw:\n        return \"\", \"\"\n\n    # Drop exchange prefix if present (e.g., \"NASDAQ:AAPL...\")\n    if \":\" in raw:\n        raw = raw.split(\":\", 1)[1].strip()\n\n    upper = raw.upper()\n\n    # Already looks like a clean ticker-only cell\n    if \" \" not in raw and len(raw) <= 8 and _TICKER_CHARS.match(upper):\n        return upper, \"\"\n\n    best_ticker, best_company, best_score = \"\", raw, -1\n\n    # Try prefixes 1..8 (covers most tickers; keeps BRK.B etc.)\n    for L in range(1, min(9, len(upper) + 1)):\n        cand = upper[:L]\n        rest = raw[L:].lstrip()\n        if not cand or not rest:\n            continue\n        if not _TICKER_CHARS.match(cand):\n            continue\n\n        score = 0\n\n        # Prefer typical lengths\n        if 1 <= L <= 5: score += 2\n        if L == 4: score += 2\n\n        # Company usually starts with a letter\n        if rest and rest[0].isalpha(): score += 2\n\n        # Company strings often contain spaces/punctuation soon\n        if \" \" in rest: score += 2\n        if \".\" in rest: score += 1\n\n        # Bonus if suffix words appear in the remainder\n        rest_tokens = re.findall(r\"[A-Za-z.]+\", rest.upper())\n        if any(t in _SUFFIX_WORDS for t in rest_tokens): score += 2\n\n        if score > best_score:\n            best_score = score\n            best_ticker, best_company = cand, rest\n\n    # Last resort: take leading ticker-like run (prevents empty)\n    if not best_ticker:\n        m = re.match(r\"([A-Z0-9.\\-=]{1,8})\", upper)\n        best_ticker = m.group(1) if m else \"\"\n\n        # Try to recover company by removing that prefix from raw\n        best_company = raw[len(best_ticker):].lstrip() if best_ticker else raw\n\n    return best_ticker, best_company\n\ndef _normalize_symbol(sym: str) -> str:\n    \"\"\"\n    Return a Yahoo-compatible ticker from any raw TradingView symbol cell.\n    This is the one you should use everywhere downstream.\n    \"\"\"\n    ticker, _company = _split_symbol_cell(sym)\n    return ticker.strip().upper()\n\n# -------------------------\n# TradingView fetch (same protocol: pandas.read_html, plus fallback)\n# -------------------------\ndef fetch_gainers_table(url: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch TradingView market movers table using pandas.read_html (same protocol).\n    Adds a fallback path that downloads HTML then parses tables from it.\n    Also fixes 'Symbol' column by creating clean 'Ticker' and 'Company' columns and\n    overwriting 'Symbol' to be the clean ticker for downstream reliability.\n    \"\"\"\n    def _direct():\n        tables = pd.read_html(url)\n        if not tables:\n            raise ValueError(\"No tables returned by pandas.read_html.\")\n        return tables[0]\n\n    def _postprocess(df: pd.DataFrame) -> pd.DataFrame:\n        df = df.copy()\n\n        # Find a likely symbol column\n        cols_lower = [str(c).lower() for c in df.columns]\n        symbol_col = None\n        for c in df.columns:\n            cl = str(c).lower()\n            if \"symbol\" in cl or \"ticker\" in cl:\n                symbol_col = c\n                break\n        if symbol_col is None:\n            symbol_col = df.columns[0]\n\n        # Create Ticker + Company from the raw cell\n        split = df[symbol_col].apply(_split_symbol_cell)\n        df[\"Ticker\"] = split.apply(lambda x: x[0])\n        df[\"Company\"] = split.apply(lambda x: x[1])\n\n        # Make downstream usage safe: 'Symbol' becomes the clean ticker\n        df[\"Symbol\"] = df[\"Ticker\"]\n\n        # Strip whitespace just in case\n        df[\"Symbol\"] = df[\"Symbol\"].astype(str).str.strip()\n        df[\"Ticker\"] = df[\"Ticker\"].astype(str).str.strip()\n\n        return df\n\n    try:\n        df = _retry(_direct, retries=CFG.retries, backoff=CFG.backoff_seconds, what=\"TradingView read_html\")\n        return _postprocess(df)\n    except Exception as e:\n        import requests\n        headers = {\"User-Agent\": \"Mozilla/5.0 (compatible; research-notebook/1.0)\"}\n\n        def _via_requests():\n            r = requests.get(url, headers=headers, timeout=20)\n            r.raise_for_status()\n            tables = pd.read_html(r.text)\n            if not tables:\n                raise ValueError(\"No tables found in fetched HTML.\")\n            return tables[0]\n\n        logging.warning(\"Direct read_html failed; trying HTML fetch fallback. (%s)\", e)\n        df = _retry(_via_requests, retries=CFG.retries, backoff=CFG.backoff_seconds, what=\"TradingView fallback parse\")\n        return _postprocess(df)\n\n# -------------------------\n# Pick ticker helper\n# -------------------------\ndef pick_ticker_from_gainers(gainers: pd.DataFrame, rank: int = 0, fallback: str = \"AAPL\") -> str:\n    if gainers is None or gainers.empty:\n        logging.warning(\"Gainers table empty; using fallback ticker %s\", fallback)\n        return fallback\n\n    # Prefer the standardized 'Symbol' we post-processed; else hunt columns\n    if \"Symbol\" in gainers.columns:\n        symbol_col = \"Symbol\"\n    else:\n        cols = [str(c).lower() for c in gainers.columns.astype(str)]\n        symbol_col = None\n        for i, c in enumerate(cols):\n            if \"symbol\" in c or \"ticker\" in c:\n                symbol_col = gainers.columns[i]\n                break\n        if symbol_col is None:\n            symbol_col = gainers.columns[0]\n\n    rank = int(max(0, min(rank, len(gainers) - 1)))\n    raw = gainers.iloc[rank][symbol_col]\n    ticker = _normalize_symbol(raw)  # robust extraction from mashed cells\n    if not ticker:\n        ticker = fallback\n    return ticker\n\n# -------------------------\n# Yahoo history fetch (forces normalization + disk cache)\n# -------------------------\ndef fetch_yahoo_history(ticker: str, period: str, interval: str) -> pd.DataFrame:\n    \"\"\"\n    Fetch OHLCV from Yahoo Finance via yfinance, with a simple disk cache.\n    Also forces ticker normalization so we never pass \"NVDANVIDIA Corporation\" to Yahoo.\n    \"\"\"\n    raw = ticker\n    ticker = _normalize_symbol(ticker)\n    if not ticker:\n        raise ValueError(f\"Could not parse a valid ticker from: {raw!r}\")\n\n    safe = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", ticker)\n    cache_path = CACHE_DIR / f\"{safe}_{period}_{interval}.parquet\"\n\n    if cache_path.exists():\n        try:\n            df = pd.read_parquet(cache_path)\n            if df is not None and not df.empty:\n                logging.info(\"Loaded cached history: %s\", cache_path)\n                return df\n        except Exception:\n            pass  # fall through to re-download\n\n    def _download():\n        df = yf.download(\n            ticker,\n            period=period,\n            interval=interval,\n            auto_adjust=False,\n            progress=False,\n            threads=True,\n        )\n\n        if df is None or df.empty:\n            raise ValueError(f\"No data returned for {ticker}\")\n\n        df = df.copy()\n\n        # If yfinance returns MultiIndex columns, flatten them\n        if isinstance(df.columns, pd.MultiIndex):\n            df.columns = [c[0] for c in df.columns]\n\n        df.index = pd.to_datetime(df.index)\n        return df\n\n    df = _retry(_download, retries=CFG.retries, backoff=CFG.backoff_seconds, what=f\"yfinance.download({ticker})\")\n    try:\n        df.to_parquet(cache_path)\n        logging.info(\"Wrote cache: %s\", cache_path)\n    except Exception as e:\n        logging.warning(\"Could not write cache (%s). Continuing without disk cache.\", e)\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:42:54.605388Z","iopub.execute_input":"2025-12-17T05:42:54.605762Z","iopub.status.idle":"2025-12-17T05:42:54.669377Z","shell.execute_reply.started":"2025-12-17T05:42:54.605731Z","shell.execute_reply":"2025-12-17T05:42:54.667964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 1: Pull TradingView market movers and choose a ticker ---\n\ngainers_df = fetch_gainers_table(CFG.gainers_url)\ndisplay(gainers_df.head(25).sort_values('Analyst Rating',ascending=False))\n\nTICKER = pick_ticker_from_gainers(gainers_df, rank=CFG.gainer_rank, fallback=\"AAPL\")\nlogging.info(\"Selected ticker: %s\", TICKER)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:53:01.171281Z","iopub.execute_input":"2025-12-17T05:53:01.171611Z","iopub.status.idle":"2025-12-17T05:53:01.322186Z","shell.execute_reply.started":"2025-12-17T05:53:01.17158Z","shell.execute_reply":"2025-12-17T05:53:01.321422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 2: Pull Yahoo Finance history for the selected ticker ---\n\ndf = fetch_yahoo_history(TICKER, CFG.period, CFG.interval)\n\n# If yfinance returned MultiIndex columns, flatten them (can happen if it thought there were multiple tickers)\ntry:\n    import pandas as pd\n    if isinstance(df.columns, pd.MultiIndex):\n        df.columns = [c[0] for c in df.columns]\nexcept Exception:\n    pass\n\n# Standardize column names and ensure expected structure\ndf = df.rename(columns={c: str(c).title() for c in df.columns})\nrequired = {\"Open\", \"High\", \"Low\", \"Close\"}\nmissing = required - set(df.columns)\nif missing:\n    raise ValueError(f\"Missing required columns: {missing}. Available columns: {list(df.columns)}\")\n\ndf[\"SMA\"] = df[\"Close\"].rolling(window=CFG.sma_window, min_periods=1).mean()\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2025-12-17T05:55:21.70391Z","iopub.execute_input":"2025-12-17T05:55:21.704268Z","iopub.status.idle":"2025-12-17T05:55:21.729913Z","shell.execute_reply.started":"2025-12-17T05:55:21.70424Z","shell.execute_reply":"2025-12-17T05:55:21.728966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 3: Kalman smoothing (price + velocity) and simple forecast ---\n# Produces an actionable, analysis-friendly DataFrame:\n# - Smoothed price + velocity (trend)\n# - Residuals (Close - Smoothed)\n# - 1-step ahead prediction + error (out-of-sample-ish diagnostic)\n# - Forward forecast with business-day index\n# - Actionable features: returns, z-scores, bands, simple signal\n\nfrom typing import Tuple\nimport numpy as np\nimport pandas as pd\nfrom filterpy.kalman import KalmanFilter\n\ndef kalman_actionable_frame(\n    close: pd.Series,\n    n_forecast: int = 10,\n    *,\n    measurement_noise: float = 0.01,\n    process_noise_price: float = 1e-4,\n    process_noise_vel: float = 1e-4,\n    init_P: float = 1000.0,\n    band_k: float = 2.0,\n    zwin: int = 20\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    2D Kalman filter with state = [price, velocity].\n    Returns:\n      hist_df: per-date features (smoothed, velocity, residuals, 1-step pred, bands, zscores, signals)\n      fc_df:   future forecasts with predicted velocity and bands\n    \"\"\"\n    z = np.asarray(close, dtype=float).reshape(-1)\n    if len(z) < 5:\n        raise ValueError(\"Not enough data points for Kalman smoothing (need >= 5).\")\n\n    idx = close.index\n\n    kf = KalmanFilter(dim_x=2, dim_z=1)\n    kf.x = np.array([z[0], 0.0])  # [price, velocity]\n    kf.F = np.array([[1.0, 1.0],\n                     [0.0, 1.0]])\n    kf.H = np.array([[1.0, 0.0]])\n    kf.P *= float(init_P)\n    kf.R = float(measurement_noise)\n    kf.Q = np.array([[float(process_noise_price), 0.0],\n                     [0.0, float(process_noise_vel)]])\n\n    # Store filtered/smoothed state and covariance\n    xs = np.zeros((len(z), 2), dtype=float)\n    Ps = np.zeros((len(z), 2, 2), dtype=float)\n\n    # 1-step ahead predictions (prior to update) for a basic diagnostic\n    pred1 = np.full(len(z), np.nan, dtype=float)\n    pred1_var = np.full(len(z), np.nan, dtype=float)\n\n    for i, obs in enumerate(z):\n        # 1-step prior prediction for time i (except i=0 has no prior)\n        kf.predict()\n        if i > 0:\n            # predicted measurement mean and variance\n            zhat = float((kf.H @ kf.x)[0])\n            S = float((kf.H @ kf.P @ kf.H.T)[0, 0] + kf.R)\n            pred1[i] = zhat\n            pred1_var[i] = S\n\n        kf.update(obs)\n\n        xs[i] = kf.x\n        Ps[i] = kf.P\n\n    price_sm = xs[:, 0]\n    vel_sm = xs[:, 1]\n\n    # Measurement prediction (posterior) uncertainty band around smoothed state\n    # Approx: variance of measurement given posterior state/cov\n    meas_var = np.array([float((kf.H @ Ps[i] @ kf.H.T)[0, 0] + kf.R) for i in range(len(z))])\n    meas_std = np.sqrt(np.maximum(meas_var, 1e-12))\n\n    hist_df = pd.DataFrame(\n        {\n            \"Close\": close.astype(float).values,\n            \"Kalman_Price\": price_sm,\n            \"Kalman_Velocity\": vel_sm,\n            \"Residual\": close.astype(float).values - price_sm,\n            \"Pred1\": pred1,                              # 1-step ahead prediction (prior)\n            \"Pred1_Error\": close.astype(float).values - pred1,\n            \"Pred1_Std\": np.sqrt(np.maximum(pred1_var, 1e-12)),\n            \"Band_Upper\": price_sm + band_k * meas_std,\n            \"Band_Lower\": price_sm - band_k * meas_std,\n        },\n        index=idx,\n    )\n\n    # Simple actionable features\n    hist_df[\"Close_Return\"] = hist_df[\"Close\"].pct_change()\n    hist_df[\"Kalman_Return\"] = hist_df[\"Kalman_Price\"].pct_change()\n\n    # Rolling z-score of residuals (mean-reversion indicator)\n    resid_mean = hist_df[\"Residual\"].rolling(zwin, min_periods=max(5, zwin // 4)).mean()\n    resid_std = hist_df[\"Residual\"].rolling(zwin, min_periods=max(5, zwin // 4)).std()\n    hist_df[\"Residual_Z\"] = (hist_df[\"Residual\"] - resid_mean) / resid_std.replace(0, np.nan)\n\n    # Basic signal ideas (kept intentionally simple + transparent)\n    # - \"Trend\" uses velocity sign\n    # - \"Reversion\" uses residual z-score crossing thresholds\n    hist_df[\"Signal_Trend\"] = np.where(hist_df[\"Kalman_Velocity\"] > 0, 1, -1)\n    hist_df[\"Signal_Reversion\"] = 0\n    hist_df.loc[hist_df[\"Residual_Z\"] <= -2.0, \"Signal_Reversion\"] = 1   # oversold vs smoothed\n    hist_df.loc[hist_df[\"Residual_Z\"] >=  2.0, \"Signal_Reversion\"] = -1  # overbought vs smoothed\n\n    # Combine (simple priority: reversion overrides trend when extreme)\n    hist_df[\"Signal\"] = hist_df[\"Signal_Trend\"]\n    hist_df.loc[hist_df[\"Signal_Reversion\"] != 0, \"Signal\"] = hist_df[\"Signal_Reversion\"]\n\n    # --- Forecast forward by repeated predict steps (no updates) ---\n    fc_prices, fc_vels, fc_stds = [], [], []\n\n    # Use last posterior state/covariance\n    x = xs[-1].copy()\n    P = Ps[-1].copy()\n\n    for _ in range(int(n_forecast)):\n        # state prediction\n        x = kf.F @ x\n        P = kf.F @ P @ kf.F.T + kf.Q\n\n        # predicted measurement std\n        S = float((kf.H @ P @ kf.H.T)[0, 0] + kf.R)\n        fc_prices.append(float(x[0]))\n        fc_vels.append(float(x[1]))\n        fc_stds.append(float(np.sqrt(max(S, 1e-12))))\n\n    future_index = pd.bdate_range(idx[-1], periods=int(n_forecast) + 1, freq=\"B\")[1:]\n\n    fc_df = pd.DataFrame(\n        {\n            \"Kalman_Forecast\": fc_prices,\n            \"Forecast_Velocity\": fc_vels,\n            \"Forecast_Std\": fc_stds,\n            \"Forecast_Upper\": np.array(fc_prices) + band_k * np.array(fc_stds),\n            \"Forecast_Lower\": np.array(fc_prices) - band_k * np.array(fc_stds),\n        },\n        index=future_index,\n    )\n\n    return hist_df, fc_df\n\n\n# Build actionable frames\nhist_kalman, fc_kalman = kalman_actionable_frame(df[\"Close\"], n_forecast=CFG.n_forecast)\n\n# Merge key columns back into df for continuity with the rest of the notebook\ndf = df.join(hist_kalman[[\"Kalman_Price\", \"Kalman_Velocity\", \"Residual\", \"Band_Upper\", \"Band_Lower\", \"Signal\"]])\n\ndf.tail(), fc_kalman.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:55:37.206514Z","iopub.execute_input":"2025-12-17T05:55:37.206913Z","iopub.status.idle":"2025-12-17T05:55:37.25969Z","shell.execute_reply.started":"2025-12-17T05:55:37.206844Z","shell.execute_reply":"2025-12-17T05:55:37.258735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 3: Kalman smoothing (price + velocity) and simple forecast ---\n\ndef kalman_smooth_and_forecast(prices: pd.Series, n_forecast: int = 10) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"2D Kalman filter with state = [price, velocity]. Returns smoothed series and forecast series.\"\"\"\n    z = np.asarray(prices, dtype=float).reshape(-1)\n    if len(z) < 3:\n        raise ValueError(\"Not enough data points for Kalman smoothing.\")\n\n    kf = KalmanFilter(dim_x=2, dim_z=1)\n    kf.x = np.array([z[0], 0.0])  # initial state\n    kf.F = np.array([[1.0, 1.0],\n                     [0.0, 1.0]]) # transition\n    kf.H = np.array([[1.0, 0.0]]) # measurement -> price\n    kf.P *= 1000.0\n    kf.R = 0.01                   # measurement noise\n    kf.Q = np.array([[1e-4, 0.0],\n                     [0.0, 1e-4]]) # process noise\n\n    xs = np.zeros((len(z), 2), dtype=float)\n    for i, price in enumerate(z):\n        kf.predict()\n        kf.update(price)\n        xs[i] = kf.x\n\n    smoothed = pd.Series(xs[:, 0], index=prices.index, name=\"Kalman_Smoothed\")\n\n    # Forecast forward by repeated predict steps (no updates)\n    fc_vals = []\n    x = kf.x.copy()\n    for _ in range(int(n_forecast)):\n        x = kf.F @ x\n        fc_vals.append(float(x[0]))\n\n    # Business-day index aligned to last timestamp\n    future_index = pd.bdate_range(prices.index[-1], periods=int(n_forecast) + 1, freq=\"B\")[1:]\n    forecast = pd.Series(fc_vals, index=future_index, name=\"Kalman_Forecast\")\n\n    return smoothed, forecast\n\nkalman_smoothed, kalman_forecast = kalman_smooth_and_forecast(df[\"Close\"], n_forecast=CFG.n_forecast)\n\ndf[\"Kalman\"] = kalman_smoothed\ndf.tail(), kalman_forecast.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:55:46.123247Z","iopub.execute_input":"2025-12-17T05:55:46.123589Z","iopub.status.idle":"2025-12-17T05:55:46.159775Z","shell.execute_reply.started":"2025-12-17T05:55:46.123553Z","shell.execute_reply":"2025-12-17T05:55:46.158706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install -U kaleido\n# --- Step 4 ---\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=df.index, y=df[\"Close\"], mode=\"lines\", name=\"Close\"))\nfig.add_trace(go.Scatter(x=df.index, y=df[\"SMA\"], mode=\"lines\", name=f\"SMA({CFG.sma_window})\"))\nfig.add_trace(go.Scatter(x=df.index, y=df[\"Kalman\"], mode=\"lines\", name=\"Kalman Smoothed\"))\n\nfig.add_trace(go.Scatter(\n    x=kalman_forecast.index,\n    y=kalman_forecast.values,\n    mode=\"lines\",\n    name=f\"Kalman Forecast (+{CFG.n_forecast}B)\",\n    line=dict(dash=\"dash\"),\n))\n\nfig.update_layout(\n    title=f\"{TICKER} | Close + SMA + Kalman smoothing & forecast\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Price\",\n    hovermode=\"x unified\",\n    template=\"plotly_white\",\n    height=550,\n)\nfig.show(renderer=\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:55:47.142609Z","iopub.execute_input":"2025-12-17T05:55:47.142978Z","iopub.status.idle":"2025-12-17T05:55:47.178509Z","shell.execute_reply.started":"2025-12-17T05:55:47.142945Z","shell.execute_reply":"2025-12-17T05:55:47.176959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Step 5: Quick summary metrics (sanity checks) ---\n\nlatest = df.iloc[-1]\nsummary = {\n    \"Ticker\": TICKER,\n    \"Last date\": str(df.index[-1].date()),\n    \"Last close\": float(latest[\"Close\"]),\n    f\"Last SMA({CFG.sma_window})\": float(latest[\"SMA\"]),\n    \"Last Kalman\": float(latest[\"Kalman\"]),\n    \"Forecast horizon (business days)\": int(CFG.n_forecast),\n    \"Forecast last value\": float(kalman_forecast.iloc[-1]),\n}\npd.Series(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T05:55:50.092189Z","iopub.execute_input":"2025-12-17T05:55:50.092518Z","iopub.status.idle":"2025-12-17T05:55:50.102064Z","shell.execute_reply.started":"2025-12-17T05:55:50.09249Z","shell.execute_reply":"2025-12-17T05:55:50.101163Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Disclaimer\n\nThis notebook is for educational and research purposes only and is **not** financial advice.\nMarkets are risky; verify everything independently before acting.\n","metadata":{}},{"cell_type":"code","source":"# Supplementary\n# ============================================================\n# Strong-Buy Hunter: TradingView Gainers -> Yahoo OHLCV -> Kalman Smoothing Forecasts\n# Goal: ruthless throughput + clean infra + profit-oriented outputs.\n# Assumes: CFG, fetch_gainers_table(), fetch_yahoo_history(), kalman_actionable_frame() already defined above.\n# ============================================================\n\nimport math\nimport re\nimport time\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\n# -------------------------\n# Opinionated knobs (speed + reliability)\n# -------------------------\n@dataclass(frozen=True)\nclass StrongBuyRun:\n    top_n: int = 25\n    rating_col: str = \"Analyst Rating\"\n    strong_buy_match: str = \"Strong Buy\"\n\n    period: str = \"6mo\"\n    interval: str = \"1d\"\n    n_forecast: int = 10\n\n    # batching (big win): one yf.download for many tickers (then cache split)\n    batch_download: bool = True\n\n    # chart layout\n    cols: int = 2\n    height_per_row: int = 360\n    template: str = \"plotly_white\"\n\nRUN = StrongBuyRun()\n\n\n# -------------------------\n# Small utilities\n# -------------------------\ndef _clean_rating(x) -> str:\n    return \"\" if pd.isna(x) else str(x).strip()\n\ndef _is_strong_buy(rating: str, match: str) -> bool:\n    return match.lower() in _clean_rating(rating).lower()\n\ndef _ensure_close(df: pd.DataFrame) -> pd.Series:\n    if df is None or df.empty or \"Close\" not in df.columns:\n        raise ValueError(\"History frame missing Close.\")\n    close = df[\"Close\"].dropna()\n    if close.shape[0] < 10:\n        raise ValueError(\"Not enough Close data points.\")\n    return close\n\ndef _safe_ticker(t: str) -> str:\n    t = str(t).strip().upper()\n    return re.sub(r\"[^A-Z0-9.\\-_=]+\", \"\", t)\n\n\n# -------------------------\n# Strong Buy selection from TradingView table\n# -------------------------\ndef select_strong_buys(gainers_df: pd.DataFrame, run: StrongBuyRun = RUN) -> pd.DataFrame:\n    if gainers_df is None or gainers_df.empty:\n        raise ValueError(\"gainers_df is empty.\")\n\n    if run.rating_col not in gainers_df.columns:\n        raise ValueError(f\"Expected column '{run.rating_col}' not found in gainers_df.\")\n\n    top = gainers_df.head(run.top_n).copy()\n    # keep your existing sort convention (descending Analyst Rating string ordering)\n    top = top.sort_values(run.rating_col, ascending=False)\n\n    top[\"_rating\"] = top[run.rating_col].map(_clean_rating)\n    sb = top[top[\"_rating\"].map(lambda r: _is_strong_buy(r, run.strong_buy_match))].copy()\n\n    if \"Symbol\" not in sb.columns:\n        raise ValueError(\"Expected a standardized 'Symbol' column (Ticker) on the TradingView post-processed table.\")\n\n    sb[\"Ticker\"] = sb[\"Symbol\"].map(_safe_ticker)\n    sb = sb[sb[\"Ticker\"].astype(bool)].drop_duplicates(\"Ticker\")\n\n    return sb.drop(columns=[\"_rating\"])\n\n\n# -------------------------\n# Ultra-fast Yahoo history: batch download + per-ticker cache split\n# -------------------------\ndef fetch_histories_fast(\n    tickers: List[str],\n    *,\n    period: str,\n    interval: str,\n    cache_dir: Path,\n    retries: int = 3,\n    backoff: float = 1.2,\n) -> Dict[str, pd.DataFrame]:\n    \"\"\"\n    Returns dict[ticker] -> OHLCV DataFrame (Date index).\n    Strategy:\n      1) Load from parquet cache where possible\n      2) Batch-download missing tickers in one call (huge win)\n      3) Split into per-ticker frames, persist to cache\n    \"\"\"\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    def cache_path(t: str) -> Path:\n        safe = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", t)\n        return cache_dir / f\"{safe}_{period}_{interval}.parquet\"\n\n    out: Dict[str, pd.DataFrame] = {}\n    missing: List[str] = []\n\n    for t in tickers:\n        p = cache_path(t)\n        if p.exists():\n            try:\n                df = pd.read_parquet(p)\n                if df is not None and not df.empty:\n                    df.index = pd.to_datetime(df.index)\n                    out[t] = df\n                    continue\n            except Exception:\n                pass\n        missing.append(t)\n\n    if not missing:\n        return out\n\n    def _download_once() -> pd.DataFrame:\n        # yfinance supports space-separated tickers for batch download\n        joined = \" \".join(missing)\n        df = yf.download(\n            joined,\n            period=period,\n            interval=interval,\n            auto_adjust=False,\n            progress=False,\n            group_by=\"ticker\",\n            threads=True,\n        )\n        if df is None or df.empty:\n            raise ValueError(\"Batch download returned empty data.\")\n        return df\n\n    last_err = None\n    for i in range(retries):\n        try:\n            batch = _download_once()\n            break\n        except Exception as e:\n            last_err = e\n            sleep_s = backoff * (2 ** i)\n            logging.warning(\"yfinance batch download failed (%s). retry %d/%d in %.1fs\", e, i + 1, retries, sleep_s)\n            time.sleep(sleep_s)\n    else:\n        raise RuntimeError(f\"yfinance batch download failed after {retries} retries\") from last_err\n\n    # Split the batch dataframe into per-ticker frames\n    for t in missing:\n        try:\n            if isinstance(batch.columns, pd.MultiIndex):\n                # MultiIndex: (Ticker, Field)\n                if t not in batch.columns.get_level_values(0):\n                    continue\n                one = batch[t].copy()\n            else:\n                # Single ticker can come back as non-multiindex; handle gracefully\n                one = batch.copy()\n\n            if one is None or one.empty:\n                continue\n\n            one.index = pd.to_datetime(one.index)\n            out[t] = one\n\n            # persist\n            try:\n                one.to_parquet(cache_path(t))\n            except Exception as e:\n                logging.warning(\"Cache write failed for %s (%s). Continuing.\", t, e)\n\n        except Exception as e:\n            logging.warning(\"Split failed for %s (%s). Skipping.\", t, e)\n\n    return out\n\n\n# -------------------------\n# Kalman chart factory (smoothing + forecast) and a profit scoreboard row\n# -------------------------\ndef build_kalman_artifacts(\n    ticker: str,\n    hist: pd.DataFrame,\n    *,\n    n_forecast: int,\n) -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, float]]:\n    close = _ensure_close(hist)\n    hist_k, fc_k = kalman_actionable_frame(close, n_forecast=n_forecast)\n\n    # profit-oriented snapshot (simple, readable, decisive)\n    last = hist_k.iloc[-1]\n    fc_last = fc_k.iloc[-1]\n\n    forecast_return = (float(fc_last[\"Kalman_Forecast\"]) / float(last[\"Kalman_Price\"])) - 1.0\n    vol_proxy = float(hist_k[\"Close_Return\"].rolling(20).std().iloc[-1]) if hist_k.shape[0] >= 25 else float(hist_k[\"Close_Return\"].std())\n    vol_proxy = float(vol_proxy) if np.isfinite(vol_proxy) else np.nan\n\n    score = {\n        \"Forecast_Return_%\": 100.0 * forecast_return,\n        \"Velocity\": float(last[\"Kalman_Velocity\"]),\n        \"Residual_Z\": float(last.get(\"Residual_Z\", np.nan)),\n        \"20D_Vol_%\": 100.0 * vol_proxy if np.isfinite(vol_proxy) else np.nan,\n        \"Signal\": float(last.get(\"Signal\", np.nan)),\n    }\n    return hist_k, fc_k, score\n\n\ndef kalman_subplot_figure(\n    tickers: List[str],\n    histories: Dict[str, pd.DataFrame],\n    strong_buy_table: pd.DataFrame,\n    *,\n    n_forecast: int,\n    cols: int = 2,\n    template: str = \"plotly_white\",\n    height_per_row: int = 360,\n) -> Tuple[go.Figure, pd.DataFrame]:\n    \"\"\"\n    One consolidated dashboard:\n      - Subplots: Close, Kalman smoothed, Kalman forecast (dashed), bands\n      - Scoreboard: ranks by forecast return and velocity (profit + momentum)\n    \"\"\"\n    tickers = [t for t in tickers if t in histories]\n    if not tickers:\n        raise ValueError(\"No histories available for the selected tickers.\")\n\n    rows = math.ceil(len(tickers) / cols)\n    fig = make_subplots(\n        rows=rows,\n        cols=cols,\n        subplot_titles=[t for t in tickers],\n        horizontal_spacing=0.07,\n        vertical_spacing=0.11,\n    )\n    fig.update_layout(\n        template=template,\n        height=max(520, rows * height_per_row),\n        hovermode=\"x unified\",\n        title=\"Strong Buy Kalman Smoothing Forecast Dashboard (Close + Smoothed + Forward Projection)\",\n        showlegend=False,\n    )\n\n    scoreboard_rows = []\n\n    for i, t in enumerate(tickers):\n        r = (i // cols) + 1\n        c = (i % cols) + 1\n\n        hist = histories[t]\n        try:\n            hist_k, fc_k, score = build_kalman_artifacts(t, hist, n_forecast=n_forecast)\n\n            # Primary lines\n            fig.add_trace(go.Scatter(x=hist_k.index, y=hist_k[\"Close\"], mode=\"lines\", name=f\"{t} Close\"), row=r, col=c)\n            fig.add_trace(go.Scatter(x=hist_k.index, y=hist_k[\"Kalman_Price\"], mode=\"lines\", name=f\"{t} Kalman\"), row=r, col=c)\n\n            # Bands (risk envelope)\n            fig.add_trace(go.Scatter(\n                x=hist_k.index, y=hist_k[\"Band_Upper\"], mode=\"lines\", name=f\"{t} Upper\",\n                line=dict(width=1), opacity=0.35\n            ), row=r, col=c)\n            fig.add_trace(go.Scatter(\n                x=hist_k.index, y=hist_k[\"Band_Lower\"], mode=\"lines\", name=f\"{t} Lower\",\n                line=dict(width=1), opacity=0.35, fill=\"tonexty\"\n            ), row=r, col=c)\n\n            # Forecast (dashed)\n            fig.add_trace(go.Scatter(\n                x=fc_k.index, y=fc_k[\"Kalman_Forecast\"], mode=\"lines\",\n                name=f\"{t} Forecast\", line=dict(dash=\"dash\")\n            ), row=r, col=c)\n\n            # Subplot title upgrade: add the “money numbers”\n            # (Plotly subplot titles are annotations; mutate the right one.)\n            # Find the corresponding annotation index:\n            ann_idx = i\n            fr = score[\"Forecast_Return_%\"]\n            vel = score[\"Velocity\"]\n            rz = score[\"Residual_Z\"]\n            fig.layout.annotations[ann_idx].text = (\n                f\"{t} | Forecast {fr:,.2f}% | Vel {vel:,.4f} | ResidZ {rz:,.2f}\"\n            )\n\n            # add rating context\n            rating = \"\"\n            try:\n                rating = str(strong_buy_table.loc[strong_buy_table[\"Ticker\"] == t, \"Analyst Rating\"].iloc[0])\n            except Exception:\n                rating = \"Strong Buy\"\n\n            scoreboard_rows.append({\n                \"Ticker\": t,\n                \"Analyst Rating\": rating,\n                **score,\n            })\n\n        except Exception as e:\n            logging.warning(\"Skipping %s (kalman build failed: %s)\", t, e)\n\n    scoreboard = pd.DataFrame(scoreboard_rows)\n    if not scoreboard.empty:\n        # Profit-first sorting: projected upside, then trend strength, then risk (lower vol is nicer)\n        scoreboard = scoreboard.sort_values(\n            by=[\"Forecast_Return_%\", \"Velocity\", \"20D_Vol_%\"],\n            ascending=[False, False, True],\n            kind=\"mergesort\",\n        ).reset_index(drop=True)\n\n    return fig, scoreboard\n\n\n# ============================================================\n# RUN: Pull Strong Buys, download histories, render dashboard + leaderboard\n# ============================================================\n\n# 1) You already built gainers_df earlier; if not:\n# gainers_df = fetch_gainers_table(CFG.gainers_url)\n\nstrong_buys = select_strong_buys(gainers_df, RUN)\n\ndisplay(strong_buys[[\"Ticker\", \"Company\", \"Analyst Rating\"]].head(50) if {\"Company\",\"Analyst Rating\"}.issubset(strong_buys.columns) else strong_buys.head(50))\n\nstrong_buy_tickers = strong_buys[\"Ticker\"].tolist()\n\n# 2) Histories (batch + cache)\nhistories = fetch_histories_fast(\n    strong_buy_tickers,\n    period=RUN.period,\n    interval=RUN.interval,\n    cache_dir=Path(CFG.cache_dir),\n    retries=CFG.retries,\n    backoff=CFG.backoff_seconds,\n)\n\n# 3) Dashboard + profit leaderboard\nfig, leaderboard = kalman_subplot_figure(\n    strong_buy_tickers,\n    histories,\n    strong_buys,\n    n_forecast=RUN.n_forecast,\n    cols=RUN.cols,\n    template=RUN.template,\n    height_per_row=RUN.height_per_row,\n)\n\nfig.show(renderer=\"\")\n\ndisplay(leaderboard)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T06:01:40.717346Z","iopub.execute_input":"2025-12-17T06:01:40.71774Z","iopub.status.idle":"2025-12-17T06:01:44.929423Z","shell.execute_reply.started":"2025-12-17T06:01:40.717707Z","shell.execute_reply":"2025-12-17T06:01:44.928404Z"}},"outputs":[],"execution_count":null}]}